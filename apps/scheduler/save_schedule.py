import os
import uuid
import json
from datetime import datetime, timedelta, date
from dotenv import load_dotenv
import psycopg
import psycopg.rows
import pytz
import time

# Load environment variables
load_dotenv()

# Configure timezone - use system's local timezone
local_tz = pytz.timezone(time.tzname[0])

# Database connection parameters
DB_HOST = os.getenv('DB_HOST')
DB_NAME = os.getenv('DB_NAME')
DB_USER = os.getenv('DB_USER')
DB_PASSWORD = os.getenv('DB_PASSWORD')
DB_PORT = os.getenv('DB_PORT')

# Validate database connection parameters
if not all([DB_HOST, DB_NAME, DB_USER, DB_PASSWORD]):
    raise EnvironmentError("Database connection variables (DB_HOST, DB_NAME, DB_USER, DB_PASSWORD) must be set.")

# Create a connection pool
min_conn = 1
max_conn = 10 # Adjust as needed
try:
    conn_pool = psycopg.pool.SimpleConnectionPool(
        min_conn, 
        max_conn, 
        host=DB_HOST, 
        dbname=DB_NAME, 
        user=DB_USER, 
        password=DB_PASSWORD, 
        port=DB_PORT
    )
except psycopg.OperationalError as e:
    print(f"Error creating database connection pool: {e}")
    conn_pool = None # Set pool to None if creation fails

def get_connection_from_pool():
    """Gets a connection from the pool."""
    if conn_pool:
        try:
            return conn_pool.getconn()
        except psycopg.pool.PoolError as e:
            print(f"Error getting connection from pool: {e}")
            return None
    else:
        print("Connection pool is not available.")
        return None

def release_connection_to_pool(conn):
    """Releases a connection back to the pool."""
    if conn_pool and conn:
        conn_pool.putconn(conn)

def create_optimization_run(run_id: uuid.UUID):
    """
    Creates a new record in the optimization_runs table to track the run.
    
    Args:
        run_id: The unique ID for this optimization run.
    """
    conn = get_connection_from_pool()
    if not conn:
        print("Error: Database connection pool is not available for create_optimization_run.")
        return False
        
    sql = """
    INSERT INTO public.optimization_runs (id, status, orders_optimized, orders_on_time, tardiness, makespan, tasks_created)
    VALUES (%s, %s, 0, 0, 0, 0, 0) 
    ON CONFLICT (id) DO NOTHING; -- Prevents error if run_id somehow already exists
    """
    try:
        with conn.cursor(row_factory=psycopg.rows.dict_row) as cur:
            cur.execute(sql, (str(run_id), 'running'))
            conn.commit()
            print(f"Created optimization run record: {run_id}")
            return True
    except (psycopg.Error, psycopg.OperationalError) as error:
        print(f"Error creating optimization run record {run_id}: {error}")
        if conn: 
            conn.rollback()
        return False
    finally:
        if conn:
            release_connection_to_pool(conn)

def update_optimization_run(run_id: uuid.UUID, status: str, metrics: dict = None, tasks_count: int = 0, parameters: dict = None):
    """
    Updates the status and results of an optimization run in the database.
    
    Args:
        run_id: The unique ID of the optimization run to update.
        status: The final status ('completed' or 'failed').
        metrics: A dictionary containing performance metrics (tardiness, makespan, etc.).
        tasks_count: The number of tasks generated by the run.
        parameters: A dictionary containing optimization parameters, order sequence, and operation splits.
    """
    conn = get_connection_from_pool()
    if not conn:
        print("Error: Database connection pool is not available for update_optimization_run.")
        return False

    sql = """
    UPDATE public.optimization_runs
    SET 
        status = %s,
        orders_optimized = %s,
        orders_on_time = %s,
        tardiness = %s,
        makespan = %s, 
        tasks_created = %s,
        runtime_seconds = %s,
        parameters = %s::jsonb
    WHERE id = %s;
    """
    
    try:
        with conn.cursor(row_factory=psycopg.rows.dict_row) as cur:
            # Prepare metrics, providing defaults if None or key missing
            metrics = metrics or {}
            orders_optimized = metrics.get('total_orders_scheduled', 0)
            orders_on_time = metrics.get('on_time_orders', 0)
            tardiness = metrics.get('tardiness', 0.0)
            makespan = metrics.get('avg_order_makespan', 0.0)
            runtime_seconds = metrics.get('computation_time', None)

            # Convert parameters dict to JSON string if it exists
            parameters_json = json.dumps(parameters) if parameters is not None else None

            cur.execute(sql, (
                status,
                orders_optimized,
                orders_on_time,
                tardiness,
                makespan,
                tasks_count,
                runtime_seconds,
                parameters_json,  # Use the JSON string instead of raw dict
                str(run_id)
            ))
            conn.commit()
            print(f"Updated optimization run record {run_id} with status: {status}")
            return True
    except (psycopg.Error, psycopg.OperationalError) as error:
        print(f"Error updating optimization run record {run_id}: {error}")
        if conn:
            conn.rollback()
        return False
    finally:
        if conn:
            release_connection_to_pool(conn)

def save_schedule_to_db(schedule_data: list, optimization_run_id: uuid.UUID, fetched_tasks: list = None):
    """
    Saves the generated schedule to the 'tasks' table in the database using psycopg2 pool.
    Creates separate tasks for setup and operation when setup time exists.
    Combines consecutive operations that share the same machine, worker, operation, and order.
    All timestamps are stored in the local system timezone.

    Args:
        schedule_data: A list of dictionaries, where each dictionary represents a scheduled task.
        optimization_run_id: The UUID identifying the optimization run.
        fetched_tasks: Optional list of previously fetched tasks to be saved under the new run ID.
    """
    conn = get_connection_from_pool()
    if not conn:
        return {"status": "error", "message": "Database connection pool is not available."}
        
    print(f"Preparing to save schedule data for optimization run {optimization_run_id} via psycopg...")

    def convert_to_local_tz(dt):
        """Helper function to convert datetime to local timezone"""
        if dt is None:
            return None
            
        # If it's a date object, convert it to datetime at midnight in local timezone
        if isinstance(dt, date) and not isinstance(dt, datetime):
            dt = datetime.combine(dt, datetime.min.time())
            
        # If datetime is naive (no timezone info), assume it's in local time
        if dt.tzinfo is None:
            local_dt = local_tz.localize(dt)
            return local_dt
            
        # If datetime has timezone info, convert it to local time
        return dt.astimezone(local_tz)

    # First, group tasks by their key attributes
    grouped_tasks = {}
    for entry in schedule_data:
        # Create a key tuple for grouping
        key = (
            entry.get('machine_id'),
            entry.get('worker_id'),
            entry.get('operation_uuid'),
            entry.get('order_id')
        )
        if key not in grouped_tasks:
            grouped_tasks[key] = []
        grouped_tasks[key].append(entry)

    # Sort each group by start time and combine consecutive tasks
    tasks_to_insert = []
    
    # Define the columns in the order they appear in the SQL query
    columns = [
        "manufacturing_order_id", "product_id", "operation_id", "machine_id", 
        "worker_id", "quantity", "sequence", "start_time", "end_time", 
        "setup_time_mins", "required_date", "operation_name", 
        "total_duration_mins", "optimization_run_id", "type"
    ]

    # Process newly created tasks
    for key, entries in grouped_tasks.items():
        # Sort entries by start time
        entries.sort(key=lambda x: x['start_time'])
        
        current_group = []
        for entry in entries:
            if not current_group:
                current_group.append(entry)
            else:
                # Check if this entry starts immediately after the last one ends
                last_entry = current_group[-1]
                if last_entry['end_time'] == entry['start_time']:
                    # Combine with the current group
                    current_group.append(entry)
                else:
                    # Process the current group and start a new one
                    if current_group:
                        combined_entry = current_group[0].copy()
                        combined_entry['end_time'] = current_group[-1]['end_time']
                        combined_entry['total_duration_mins'] = sum(e['total_duration_mins'] for e in current_group)
                        combined_entry['setup_time_mins'] = current_group[0].get('setup_time_mins', 0)
                        
                        # Process the combined entry
                        manufacturing_order_id = uuid.UUID(combined_entry['order_id']) if combined_entry.get('order_id') else None
                        product_id = uuid.UUID(combined_entry['product_id']) if combined_entry.get('product_id') else None
                        operation_uuid = combined_entry.get('operation_uuid')
                        machine_id = uuid.UUID(combined_entry['machine_id']) if combined_entry.get('machine_id') else None
                        worker_id = uuid.UUID(combined_entry['worker_id']) if combined_entry.get('worker_id') else None
                        worker_required = combined_entry.get('worker_required', 'SetupAndOperation')  # Default to both if not specified
                        
                        setup_time_mins = int(combined_entry['setup_time_mins']) if combined_entry.get('setup_time_mins') is not None else 0
                        total_duration_mins = float(combined_entry['total_duration_mins'])
                        operation_duration_mins = total_duration_mins - setup_time_mins

                        # If there's setup time, create a setup task
                        if setup_time_mins > 0:
                            setup_end_time = combined_entry['start_time'] + timedelta(minutes=setup_time_mins)
                            
                            # Only include worker in setup task if they're required for setup
                            setup_worker_id = str(worker_id) if worker_id and worker_required in ["SetupOnly", "SetupAndOperation"] else None
                            
                            setup_task = (
                                str(manufacturing_order_id) if manufacturing_order_id else None,
                                str(product_id) if product_id else None,
                                str(operation_uuid) if operation_uuid else None,
                                str(machine_id) if machine_id else None,
                                setup_worker_id,  # Only include worker if required for setup
                                int(combined_entry['quantity']) if combined_entry.get('quantity') is not None else None,
                                int(combined_entry['sequence_number']) if combined_entry.get('sequence_number') is not None else None,
                                combined_entry['start_time'],
                                setup_end_time,
                                setup_time_mins,
                                combined_entry.get('required_date'),
                                combined_entry.get('operation_name'),
                                float(setup_time_mins),
                                str(optimization_run_id),
                                'setup'
                            )
                            tasks_to_insert.append(setup_task)
                            operation_start_time = setup_end_time
                        else:
                            operation_start_time = combined_entry['start_time']

                        # Only include worker in operation task if they're required for operation
                        operation_worker_id = str(worker_id) if worker_id and worker_required in ["OperationOnly", "SetupAndOperation"] else None

                        # Create the operation task
                        operation_task = (
                            str(manufacturing_order_id) if manufacturing_order_id else None,
                            str(product_id) if product_id else None,
                            str(operation_uuid) if operation_uuid else None,
                            str(machine_id) if machine_id else None,
                            operation_worker_id,  # Only include worker if required for operation
                            int(combined_entry['quantity']) if combined_entry.get('quantity') is not None else None,
                            int(combined_entry['sequence_number']) if combined_entry.get('sequence_number') is not None else None,
                            operation_start_time,
                            combined_entry['end_time'],
                            0,  # No setup time for operation task
                            combined_entry.get('required_date'),
                            combined_entry.get('operation_name'),
                            float(operation_duration_mins),
                            str(optimization_run_id),
                            'operation'
                        )
                        tasks_to_insert.append(operation_task)
                    
                    current_group = [entry]
            
        # Process the last group
        if current_group:
            combined_entry = current_group[0].copy()
            combined_entry['end_time'] = current_group[-1]['end_time']
            combined_entry['total_duration_mins'] = sum(e['total_duration_mins'] for e in current_group)
            combined_entry['setup_time_mins'] = current_group[0].get('setup_time_mins', 0)
            
            # Process the combined entry
            manufacturing_order_id = uuid.UUID(combined_entry['order_id']) if combined_entry.get('order_id') else None
            product_id = uuid.UUID(combined_entry['product_id']) if combined_entry.get('product_id') else None
            operation_uuid = combined_entry.get('operation_uuid')
            machine_id = uuid.UUID(combined_entry['machine_id']) if combined_entry.get('machine_id') else None
            worker_id = uuid.UUID(combined_entry['worker_id']) if combined_entry.get('worker_id') else None
            worker_required = combined_entry.get('worker_required', 'SetupAndOperation')  # Default to both if not specified
            
            setup_time_mins = int(combined_entry['setup_time_mins']) if combined_entry.get('setup_time_mins') is not None else 0
            total_duration_mins = float(combined_entry['total_duration_mins'])
            operation_duration_mins = total_duration_mins - setup_time_mins

            # If there's setup time, create a setup task
            if setup_time_mins > 0:
                setup_end_time = combined_entry['start_time'] + timedelta(minutes=setup_time_mins)
                
                # Only include worker in setup task if they're required for setup
                setup_worker_id = str(worker_id) if worker_id and worker_required in ["SetupOnly", "SetupAndOperation"] else None
                
                setup_task = (
                    str(manufacturing_order_id) if manufacturing_order_id else None,
                    str(product_id) if product_id else None,
                    str(operation_uuid) if operation_uuid else None,
                    str(machine_id) if machine_id else None,
                    setup_worker_id,  # Only include worker if required for setup
                    int(combined_entry['quantity']) if combined_entry.get('quantity') is not None else None,
                    int(combined_entry['sequence_number']) if combined_entry.get('sequence_number') is not None else None,
                    combined_entry['start_time'],
                    setup_end_time,
                    setup_time_mins,
                    combined_entry.get('required_date'),
                    combined_entry.get('operation_name'),
                    float(setup_time_mins),
                    str(optimization_run_id),
                    'setup'
                )
                tasks_to_insert.append(setup_task)
                operation_start_time = setup_end_time
            else:
                operation_start_time = combined_entry['start_time']

            # Only include worker in operation task if they're required for operation
            operation_worker_id = str(worker_id) if worker_id and worker_required in ["OperationOnly", "SetupAndOperation"] else None

            # Create the operation task
            operation_task = (
                str(manufacturing_order_id) if manufacturing_order_id else None,
                str(product_id) if product_id else None,
                str(operation_uuid) if operation_uuid else None,
                str(machine_id) if machine_id else None,
                operation_worker_id,  # Only include worker if required for operation
                int(combined_entry['quantity']) if combined_entry.get('quantity') is not None else None,
                int(combined_entry['sequence_number']) if combined_entry.get('sequence_number') is not None else None,
                operation_start_time,
                combined_entry['end_time'],
                0,  # No setup time for operation task
                combined_entry.get('required_date'),
                combined_entry.get('operation_name'),
                float(operation_duration_mins),
                str(optimization_run_id),
                'operation'
            )
            tasks_to_insert.append(operation_task)

    # Process fetched tasks if provided
    if fetched_tasks:
        print(f"Processing {len(fetched_tasks)} fetched tasks to save under new optimization run ID")
        for task in fetched_tasks:
            # Only process tasks that have a manufacturing order ID
            if task.get('manufacturing_order_id'):
                # Create a new task tuple with the new optimization run ID
                fetched_task = (
                    task.get('manufacturing_order_id'),
                    task.get('product_id'),
                    task.get('operation_id'),
                    task.get('machine_id'),
                    task.get('worker_id'),
                    task.get('quantity'),
                    task.get('sequence'),
                    task.get('start_time'),
                    task.get('end_time'),
                    task.get('setup_time_mins'),
                    task.get('required_date'),
                    task.get('operation_name'),
                    task.get('total_duration_mins'),
                    str(optimization_run_id),
                    task.get('type')
                )
                tasks_to_insert.append(fetched_task)

    if not tasks_to_insert:
        print("No tasks to insert.")
        return {"status": "warning", "message": "No tasks generated to save."}

    try:
        with conn.cursor(row_factory=psycopg.rows.dict_row) as cur:
            # Construct the SQL query for bulk insert
            sql = f"""
            INSERT INTO public.tasks ({', '.join(columns)})
            VALUES %s
            """
            
            print(f"Inserting {len(tasks_to_insert)} tasks into the database using executemany...")
            
            # Execute the bulk insert
            cur.executemany(sql, tasks_to_insert)

            # Commit the transaction
            conn.commit()
            
            print(f"Successfully inserted {len(tasks_to_insert)} tasks.")
            return {"status": "success", "message": f"Successfully inserted {len(tasks_to_insert)} tasks."}

    except (psycopg.Error, psycopg.OperationalError, psycopg.pool.PoolError) as error:
        print(f"Error inserting tasks into database: {error}")
        if conn: # Rollback if connection exists
            conn.rollback()
        return {"status": "error", "message": f"Database insertion error: {error}"}
        
    finally:
        if conn:
            release_connection_to_pool(conn) # Release connection back to the pool

# Example usage (optional, for testing)
if __name__ == '__main__':
    # Ensure .env is loaded for testing
    if not all([DB_HOST, DB_NAME, DB_USER, DB_PASSWORD]):
       print("Database environment variables not set. Skipping direct execution example.")
    elif not conn_pool:
       print("Database connection pool failed to initialize. Skipping direct execution example.")
    else:
        # Create some dummy data for testing
        test_run_id = uuid.uuid4()
        test_schedule = [
            {
                'order_id': str(uuid.uuid4()), 
                'product_id': str(uuid.uuid4()), 
                'operation_uuid': uuid.uuid4(), # Use UUID object
                'operation_name': 'Test Op 1 Psycopg2', 
                'sequence_number': 1, 
                'unit_idx': 0, # Not used directly in insert, but part of original data
                'part_idx': 0, # Not used directly in insert
                'start_time': datetime.now(), 
                'end_time': datetime.now() + timedelta(hours=2), 
                'machine_id': str(uuid.uuid4()), 
                'worker_id': str(uuid.uuid4()), 
                'quantity': 10,
                'setup_time_mins': 30,
                'total_duration_mins': 120.5, # Example float
                'required_date': datetime.now() + timedelta(days=5)
            }
            # Add more entries if needed
        ]
        
        result = save_schedule_to_db(test_schedule, test_run_id)
        print(f"Save operation result: {result}")

        # Clean up the connection pool (optional)
        if conn_pool:
            conn_pool.closeall()
            print("Connection pool closed.") 